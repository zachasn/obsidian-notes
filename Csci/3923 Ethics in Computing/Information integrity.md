# Information integrity
An initial promise of the personal computing, the Internet, and social media was to bring information to people. Never before have we have we had easy access to so much information.

And Yet, has this helped us?

## Yes and No 
Some issues:  
-  Overall contribution: Overall, how much has so much information led to positive individual and societal benefits?  
- Quantity: The “firehose” problem — too much information. 
- Veracity: Not all information is true. Some is just inaccurate; some is posted by bad actors to try to mislead people.  
- Relevance: Not all information is helpful. (low “signal to noise ratio”)
- Information quality: something might be true and relevant but just not informative, persuasive, useful, etc.  
- Social Media Algorithms: Various issues including creating echo chambers, promoting material that feeds into our own biases, promoting more sensational but less useful information, ...
- Difference between information and wisdom: there is much information, but not as much wisdom.
• ...
## Disinformation
Not everyone online is your friend. There are people and organizations who — for a variety of reasons including financial, political, and personal ones — will try to convince you to believe and/or act on information that is untrue. Bad Actors? Of many types (and soften non-transparently)
- Individuals (and for a variety of reasons including personal gain, promoting various causes, etc.) or small groups of individuals
- Companies or organizations behaving badly (e.g., Cambridge Analytica)
- Nation-states behaving badly
- State-sponsored or backed individuals or groups

Generative AI makes it much easier for both bad actors and careless actors to make deep fakes, inaccurate but authoritative looking accounts of events, etc.
##### propaganda techniques
- Distraction (“Look over there!”, “But what about ...”, ...)
- Having the the aim of provoking people, but then disavowing that intention (e.g., “I was just joking”)  
- Untrue “evidence” (e.g., deepfakes)
- Ad hominem attacks: attacking people rather than their ideas.
- Cherry-picking evidence, confirmation bias, etc. to promote one’s agenda or to denigrate an opponent’s.  
- Arguing an alternative (or all alternatives) are poor choices because they have some undesirable characteristics (it is easier to be against something than for something (why?))
- “Gaslighting”: Manipulating someone into questioning their own beliefs or reasoning.  
- Use of “truth as a weapon” (e.g., to demean rather than inform).  
- Use of proxies (for a variety of reasons)
- Astroturfing: facilitating (either actual or ersatz) grassroots efforts in support of one’s agenda.
- Flooding to create uncertainty: creating and promoting so many different narratives that which one(s) is accurate is difficult to determine.  
- Counternarratives: Creating and promoting positive stories to drown out negative ones; creating and promoting negative stories to drown out positive ones.  
- Use of logical fallacies
## Logical Fallacies
- **many-any fallacy: **Assuming that because many instances of something exist, any particular instance must have the same characteristics.
	- *Example*: "Many tech companies have been caught selling user data, so this new startup must be selling user data too."
- **either-or fallacy:** Presenting only two options when more alternatives exist.
	- *Example*: "Either we allow complete freedom on social media platforms, or we live in a censored dystopia."
- **All-or-nothing fallacy (similar to either-or fallacy):** Arguing that a solution must be perfect in order to be useful.
	- *Example*: Because the Internet is decentralized, crosses international boundaries, etc. it's impossible for any Internet regulations to be 100% effective, and therefore the Internet should not be regulated. → disregards the fact that even if a regulation is not 100% effective, it might nonetheless promote some desired behavior and deter some unwanted behavior. 
- **false cause fallacy:** Assuming that because two events occur together or in sequence, one must have caused the other.
	- *Example*: "After we increased social media usage in schools, test scores declined. Therefore, social media causes poor academic performance." (ignores other potential factors)
- **ad hominem argument:** Attacking the person making an argument rather than addressing the argument itself.
	- *Example*: "You can't trust her views on AI ethics—she doesn't even have a computer science degree."
- **Confirmation bias:** The tendency to search for, interpret, or recall information in a way that confirms pre-existing beliefs.
	- *Example*: Only reading news articles that support your political views while dismissing contradictory evidence as "fake news."
- **Hindsight bias:** The tendency to believe, after an outcome is known, that one would have predicted it beforehand.
	- *Example*: "I knew that cryptocurrency was going to crash—it was so obvious all along." (when you made no such prediction before the crash)
- **virtuality fallacy:** Assuming that actions in virtual/online spaces have no real-world consequences or moral weight.
	- *Example*: "It's just online harassment—it's not real violence, so it doesn't really hurt anyone."
- **Ambiguity/Equivocation fallacy:** Occurs when terms are not used precisely.
	- *Example*: "People should not have concerns about privacy because government surveillance is necessary for national security reasons." → This is defining privacy solely with respect to government surveillance for national security.
- **Appeal to authority fallacy:** Claims that because an authoritative person or  organization says something, it must be true.
	- *Example*: "The NFL (National Football League) claims that any reuse of a football telecast is illegal, unless it is authorized by the NFL. Therefore any non-authorized Internet posting of any part of a NFL football telecast is illegal."
- **Appeal to the people fallacy:** Arguing that something is true or right because many people believe it (argumentum ad populum).
	- *Example*: "Millions of people use this app without reading the privacy policy, so it must be fine to ignore it."
- **Begging the question:** Using circular reasoning where the conclusion is assumed in the premise.
	- *Example*: "This encryption algorithm is secure because it uses unbreakable encryption." (assumes what needs to be proven)
- **Composition/Division fallacy:** Assuming what is true of parts is true of the whole (composition), or what is true of the whole is true of parts (division).
	- *Example*: "Each component of this software system is secure, therefore the entire system must be secure." (ignores integration vulnerabilities)
- **Dunning-Kruger Effect:** When people with low competence in a domain overestimate their ability or knowledge.
	- *Example*: A novice programmer claiming "cybersecurity is easy—just use strong passwords" after reading one blog post.
- Eyewitness fallacy: Over reliance on eyewitness testimony despite its known unreliability.
	- *Example*: "I saw the notification pop up on my screen, so I'm certain my account was hacked." (could be phishing, UI glitch, or misinterpretation)
- **Fallacy fallacy:** Assuming that because an argument contains a fallacy, the conclusion must be false.
	- *Example*: "Your argument for data privacy used an appeal to emotion, therefore data privacy isn't important."
- **Prosecutor's fallacy:** Confusing the probability of evidence given guilt with the probability of guilt given evidence.
	- *Example*: "Only 1% of legitimate emails contain this phrase, and this email contains it, so there's a 99% chance it's spam." (ignores base rate of spam vs. legitimate emails)
- **Single cause fallacy:** Attributing a complex phenomenon to a single cause when multiple factors are involved.
	- *Example*: "The data breach happened because the password was weak." (ignores potential issues with network security, employee training, system vulnerabilities, etc.)
- **Slippery slope fallacy:** Arguing that one action will inevitably lead to a chain of events resulting in an extreme outcome, without justification.
	- *Example*: "If we require software companies to patch security vulnerabilities within 30 days, soon the government will control all aspects of software development."