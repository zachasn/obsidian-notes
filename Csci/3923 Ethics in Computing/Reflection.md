# Reflection
### Questions
1. What is one important thing I learned about ethics and computing? And why is it important? (“Computing” here means generative AI)  
2. What topic(s) did I personally find most interesting  and/or important? Why?  
3. What wasn't covered that I would have liked to have seen, and why? (Or what was covered, but not in the depth I would have liked?)  
4. What  is one important thing you learned from others in the class?  
5. Did you find the in-class time useful? If yes, what made it useful for you? If not, how could the class be to be more useful?  
6. Did you find the between-class work useful? If yes, what made it useful for you? If  not, how could it be changed to be more useful?  
7. What is one specific thing that I learned that I think will be  important to remember, and why? For instance, what would I like to remember a  year from now about the material?
8. What other question do I wish the instructor would have asked? Make up your own  question, on anything having to do with the class, and then answer it.  
9. Is there anything else you’d like me know to about what you learned, how you found the class, etc.?
10. What is one important thing you learned about ethics and computing? And why is it important? (“Computing” here means security\privacy )
### Answers

1. The most important thing I learned is that LLMs are just pattern matching text generators, they're not sentient and can't meaningfully assess their own capabilities. This matters because it changes how we should actually use these tools, we can't treat their outputs as trustworthy or assume they understand context like humans do. The transformer architecture uses attention mechanisms to identify relevant words and feedforward layers to "think" about relationships, but that's still just statistical pattern matching, not real reasoning. Understanding this stops us from attributing human-like qualities to AI systems and reminds us that we're still responsible for verifying and evaluating what they generate.

2. I personally found AI & Learning most interesting because it hits directly at how I approach my own education. The research showing that students often over-rely on AI while thinking it helps them learn more efficiently really stuck with me - it's hard to evaluate in the moment whether AI is actually helping or hurting your learning. The idea that "struggle is key to learning" challenged my tendency to use AI as a shortcut. Learning isn't just about acquiring facts, it's about developing critical thinking, problem-solving skills, communication abilities, and understanding different terminology and perspectives. The distinction between process and product really hit home for me - sometimes the struggle of working through a problem is way more valuable than just quickly getting to the answer.

3. I would've liked more coverage of **AI security vulnerabilities** and their ethical implications. We touched on prompt injections, but I wanted to go deeper into adversarial attacks, model extraction, data poisoning, and the tension between model transparency (which you need for fairness and accountability) and model security (preventing exploitation). The ethics of open-source versus closed AI models would also have been interesting - what are the actual tradeoffs between democratizing access to powerful AI systems and the increased risks of misuse? Understanding these security dimensions seems crucial for building responsible AI systems.

4. During discussions about [[Algorithm fairness]], I learned from classmates that there are actually multiple incompatible definitions of fairness. Some people emphasized statistical parity while others argued for equal opportunity or individual fairness. This diversity of perspectives really helped me see that "being fair" isn't a straightforward technical problem - different stakeholders can have legitimate but conflicting views on what fairness actually means. Hearing classmates from different backgrounds explain how algorithmic systems had affected them or their communities made the abstract concepts way more concrete and reinforced that algorithmic fairness needs input from all affected stakeholders, not just technical experts.

5. In-class time was definitely useful because applying ethical frameworks to real case studies made the abstract theories actually concrete. Analyzing dark UX patterns through Deontology versus Utilitarianism lenses showed me that different frameworks reveal different ethical dimensions of the same problem. The small group discussions where we had to defend positions we might not personally hold were particularly valuable - they forced me to understand multiple perspectives and see that ethical decisions usually involve tradeoffs rather than clear right answers. These exercises developed my ability to actually think critically about computing ethics instead of just relying on intuition.

6. The Weekly Reading Notes were useful because the structured reflection questions pushed me to actually synthesize information instead of just passively consuming it. Prompts like "What should the public know about how generative AI works?" forced me to identify what's genuinely important versus just interesting. Connecting the readings to personal experience definitely made the material stick better. That said, I wish we'd had more technical deep-dives into how specific systems actually work - understanding the mechanisms (like how word vectors enable reasoning in LLMs or how training data biases propagate through models) really strengthens ethical analysis because you can identify where problems originate and how to actually address them.

7. A year from now, I want to remember that technomoral wisdom requires cultivating both technical knowledge and social understanding. Shannon Vallor's framework really resonated because ethical computing isn't just about following rules or having good intentions, you need to understand how technology actually works (LLM architectures, algorithmic decision-making, data inference mechanisms) and the social context (stakeholder impact, power dynamics, downstream consequences). When I'm facing decisions about what to build or how to build it, I'll remember that addressing challenges like algorithmic bias requires both technical solutions (fixing data, adjusting models) and social solutions (stakeholder consultation, democratic input on fairness definitions).

8. I initially resisted the idea that algorithms could be systematically biased, I thought careful developers could just avoid bias if they tried. But learning about the multiple error sources (incorrect data, unrepresentative training sets, poor model design, spurious correlations, inappropriate use contexts) showed me that bias isn't just oversight, it's often structural. The fact that technology isn't actually neutral - design choices are "baked in" challenged my assumption that code is objective. Understanding that there are multiple definitions of fairness really drove home that algorithmic fairness requires value judgments and democratic input, not just technical skill. This whole evolution made me way more humble about what technical solutions alone can actually accomplish.

9. This course really revealed that ethical problems in computing are often **wicked problems** with no perfect solutions, just tradeoffs. For instance, greater algorithmic transparency helps with accountability but can enable adversarial attacks. Comprehensive data collection improves AI performance but threatens privacy. Open-source AI democratizes access but increases misuse risks. Initially, I expected to find clear guidelines for ethical computing, but I learned that context matters enormously - the same tool can be ethical in one application and harmful in another. This means computing professionals need judgment and ongoing deliberation with stakeholders instead of just following rules, which requires way more active engagement than I initially thought.

10. 