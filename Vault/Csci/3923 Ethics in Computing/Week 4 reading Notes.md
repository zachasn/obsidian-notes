## Readings
1. ChatGPT is one year old. Here's how it changed the tech world. [Link](https://arstechnica.com/information-technology/2023/11/chatgpt-was-the-spark-that-lit-the-fire-under-generative-ai-one-year-ago-today/)
2. Why it's a mistake to ask chatbots about their mistakes. [Link](https://arstechnica.com/ai/2025/08/why-its-a-mistake-to-ask-chatbots-about-their-mistakes/)
3. A jargon-free explanation of how AI large language models work. [Link](https://arstechnica.com/science/2023/07/a-jargon-free-explanation-of-how-ai-large-language-models-work/)
## Summaries
The first article discusses the impact of ChatGPT on the tech industry over the past year, highlighting how it has changed people's perception of AI, spurred innovation, and influenced various sectors. It also goes into details about the development and deployment of ChatGPT.

The second article is about the challenges of using AI chatbots to identify and correct their own mistakes. It gives 2 main reasons why this approach is flawed:
1. You're not talking to a person or an entity with a consistent identity when you interact with a chatbot. You're talking to a system that generates plausible-sounding text based on patterns in its training data, not a self-aware being that can reflect on its own actions.
2. LLMs cannot meaningfully assess their own capabilities. Unlike humans, who can introspect and asses their own knowledge, AI models don't have a stable, accessible knowledge base. Instead, what they "know" is distributed across billions of parameters, making it impossible for them to accurately evaluate their own performance.

The third article is about the inner workings of large language models. It first explains word vectors, which is how LLMs represent words as a long list of numbers. LLM's do this because it allows them to reason about the relationships between words in a mathematical way. The article then talks about transformers, which are the architecture that LLMs use to process and generate text. Each transformer has a two step process for updating what it knows about the input text: attention and feedforward. In the attention step, the model looks at all the words in the input text and decides which ones are most relevant to the current word it's processing. In the feedforward step, the model "thinks" about the current word and the relevant words it identified in the attention step, and updates its understanding of the input text accordingly. The article also discusses how LLMs are trained using a process called back propagation, which involves adjusting the model's parameters based on how well it predicts the next word in a sequence. It also talks about how today's LLMs don't need explicitly labeled training data, but instead learn from the patterns in the text they are trained on.

## Takeaway
1. The hype around LLMs has definitely changed the tech landscape, but it's important to remember that they are still tools with limitations. They can be incredibly useful for certain tasks, but they are not sentient beings and should not be treated as such.
2. LLMs can have some use if you treat them as what they are: tools that make things up based on patterns. If you treat them as knowledgeable entities, you're likely to be misled. This is especially true when it comes to self-assessment. Just because a chatbot says "I think I made a mistake" doesn't mean it actually understands what a mistake is or can reliably identify one. It's just generating text that sounds like something a person might say in that situation.
3. The inner workings of LLMs are very complex, with even researchers struggling to fully understand how they work. This makes it difficult to predict their behavior and ensure they are used safely and ethically. As LLMs become more prevalent, it's crucial for users to have a basic understanding of how they function and their limitations.

4. The two most important things I have learned about how generative AI works are word vectors and transformers. Word vectors allow LLMs to represent and reason about words using vector arithmetics, which is crucial for understanding relationships between words. Transformers are how LLMs process and generate text. LLMs use many layers of transformers to refine their understanding of the input text and generate coherent output. Each transformer has a two step process for updating what it knows about the input text, attention and feedforward. In the attention step, the model looks at all the words in the input text and decides which ones are most relevant to the current word it's processing. In the feedforward step, the model "thinks" about the current word and the relevant words it identified in the attention step, and updates its understanding of the input text accordingly.

5. The two most important things I think the public should know about how generative AI works are that LLMs are not sentient beings and that they cannot meaningfully assess their own capabilities. It's important for the public to understand that LLMs are tools that generate text based on patterns in their training data, not self-aware entities. This understanding can help prevent people from attributing human-like qualities to these models and expecting them to behave like humans. Additionally, knowing that LLMs cannot accurately evaluate their own performance can help users be more critical of the information generated by these models and avoid being misled.